{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "import libcst as cst\n",
    "from tqdm import tqdm\n",
    "\n",
    "from typet5.data import GitRepo, get_dataset_dir\n",
    "from typet5.type_env import collect_annots_info, mypy_checker\n",
    "from typet5.utils import proj_root, read_file, write_file, not_none\n",
    "\n",
    "os.chdir(proj_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repos already downloaded.\n",
      "Reading last updates...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 898/3644 [00:11<00:35, 76.79it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mReading last updates...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m tqdm(downloaded_repos):\n\u001b[0;32m---> 51\u001b[0m         r\u001b[39m.\u001b[39;49mread_last_update(repos_dir)\n\u001b[1;32m     52\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloaded \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(downloaded_repos)\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(all_repos)\u001b[39m}\u001b[39;00m\u001b[39m repos.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/sem4/NLP/Final/TypeT5/src/typet5/data.py:84\u001b[0m, in \u001b[0;36mGitRepo.read_last_update\u001b[0;34m(self, repos_dir)\u001b[0m\n\u001b[1;32m     80\u001b[0m s \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mrun(\n\u001b[1;32m     81\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mgit\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mlog\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m--format=\u001b[39m\u001b[39m%c\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m], cwd\u001b[39m=\u001b[39md, capture_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, text\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     82\u001b[0m )\u001b[39m.\u001b[39mstdout\n\u001b[1;32m     83\u001b[0m lu \u001b[39m=\u001b[39m dateparser\u001b[39m.\u001b[39mparse(s\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m])\n\u001b[0;32m---> 84\u001b[0m \u001b[39massert\u001b[39;00m lu \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_update \u001b[39m=\u001b[39m lu\u001b[39m.\u001b[39mreplace(tzinfo\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_update\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# download all candidate repos\n",
    "\n",
    "all_repos = json.loads(read_file(\"data/mypy-dependents-by-stars.json\"))\n",
    "all_repos = [GitRepo.from_json(r) for r in all_repos]\n",
    "# all_repos=all_repos[:10] # for testing\n",
    "\n",
    "repos_dir = get_dataset_dir(\"ManyTypes4Py\") / \"repos\"\n",
    "\n",
    "def clear_downloaded_repos(repos_dir):\n",
    "    shutil.rmtree(repos_dir)\n",
    "\n",
    "\n",
    "def download_repos(\n",
    "    to_download: list[GitRepo], repos_dir, download_timeout=10.0, max_workers=10\n",
    ") -> list[GitRepo]:\n",
    "    def download_single(repo: GitRepo):\n",
    "        try:\n",
    "            if repo.download(repos_dir, timeout=download_timeout):\n",
    "                repo.read_last_update(repos_dir)\n",
    "                return repo\n",
    "            else:\n",
    "                return None\n",
    "        except subprocess.TimeoutExpired:\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to download {repo.name}. Exception: {e}\")\n",
    "            return None\n",
    "\n",
    "    print(\"Downloading repos from Github...\")\n",
    "    t_start = time.time()\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        fs = [executor.submit(download_single, repo) for repo in to_download]\n",
    "        rs = [f.result() for f in tqdm(as_completed(fs), total=len(fs))]\n",
    "    print(f\"Downloading took {time.time() - t_start} seconds.\")\n",
    "    downloaded = [r for r in rs if r is not None]\n",
    "    return downloaded\n",
    "\n",
    "\n",
    "if not repos_dir.exists():\n",
    "    (repos_dir / \"downloading\").mkdir(parents=True)\n",
    "    (repos_dir / \"downloaded\").mkdir(parents=True)\n",
    "    downloaded_repos = download_repos(all_repos, repos_dir)\n",
    "    print(\"Deleting failed repos...\")\n",
    "    shutil.rmtree(repos_dir / \"downloading\")\n",
    "else:\n",
    "    print(\"Repos already downloaded.\")\n",
    "    downloaded_dirs = set(d.name for d in (repos_dir / \"downloaded\").iterdir())\n",
    "    downloaded_repos = [r for r in all_repos if r.authorname() in downloaded_dirs]\n",
    "    print(\"Reading last updates...\")\n",
    "    for r in tqdm(downloaded_repos):\n",
    "        r.read_last_update(repos_dir)\n",
    "print(f\"Downloaded {len(downloaded_repos)}/{len(all_repos)} repos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, timezone\n\u001b[1;32m      5\u001b[0m date_threshold \u001b[39m=\u001b[39m datetime(\u001b[39m2021\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m new_repos \u001b[39m=\u001b[39m [r \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m downloaded_repos \u001b[39mif\u001b[39;00m not_none(r\u001b[39m.\u001b[39mlast_update) \u001b[39m>\u001b[39m date_threshold]\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(new_repos)\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(downloaded_repos)\u001b[39m}\u001b[39;00m\u001b[39m repos are updated within a year.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m loc_limit \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m\n",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatetime\u001b[39;00m \u001b[39mimport\u001b[39;00m datetime, timezone\n\u001b[1;32m      5\u001b[0m date_threshold \u001b[39m=\u001b[39m datetime(\u001b[39m2021\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m20\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m new_repos \u001b[39m=\u001b[39m [r \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m downloaded_repos \u001b[39mif\u001b[39;00m not_none(r\u001b[39m.\u001b[39;49mlast_update) \u001b[39m>\u001b[39m date_threshold]\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(new_repos)\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(downloaded_repos)\u001b[39m}\u001b[39;00m\u001b[39m repos are updated within a year.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m loc_limit \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/sem4/NLP/Final/TypeT5/src/typet5/utils.py:231\u001b[0m, in \u001b[0;36mnot_none\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnot_none\u001b[39m(x: Optional[T1]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T1:\n\u001b[0;32m--> 231\u001b[0m     \u001b[39massert\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# filter out repos that are too old or too big\n",
    "\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "date_threshold = datetime(2021, 4, 20)\n",
    "new_repos = [r for r in downloaded_repos if not_none(r.last_update) > date_threshold]\n",
    "print(f\"{len(new_repos)} / {len(downloaded_repos)} repos are updated within a year.\")\n",
    "loc_limit = 50000\n",
    "\n",
    "small_repos = []\n",
    "for rep in tqdm(new_repos):\n",
    "    try:\n",
    "        loc = rep.count_lines_of_code(repos_dir)\n",
    "        if loc < loc_limit:\n",
    "            small_repos.append(rep)\n",
    "    except UnicodeDecodeError:\n",
    "        # nothing we can do\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to count lines of code for {rep.name}. Exception: {e}\")\n",
    "\n",
    "print(\n",
    "    f\"{len(small_repos)}/{len(new_repos)} repos are within the size limit ({loc_limit} LOC).\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter away repos with too few annotations\n",
    "# import sys\n",
    "# sys.path.append(\"../src/\")\n",
    "def count_repo_annots(rep):\n",
    "    try:\n",
    "        rep.count_annotations(repos_dir)\n",
    "        if rep.n_type_annots / rep.lines_of_code > 0.05:\n",
    "            return rep\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"Failed to count annotations for {rep.name}. Exception: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_repos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfuns_new\u001b[39;00m \u001b[39mimport\u001b[39;00m count_repo_annots\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m) \u001b[39mas\u001b[39;00m executor:\n\u001b[0;32m----> 4\u001b[0m     fs \u001b[39m=\u001b[39m [executor\u001b[39m.\u001b[39msubmit(count_repo_annots, rep, repos_dir) \u001b[39mfor\u001b[39;00m rep \u001b[39min\u001b[39;00m small_repos]\n\u001b[1;32m      5\u001b[0m     rs \u001b[39m=\u001b[39m [f\u001b[39m.\u001b[39mresult() \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tqdm(as_completed(fs), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(fs))]\n\u001b[1;32m      6\u001b[0m useful_repos: \u001b[39mlist\u001b[39m[GitRepo] \u001b[39m=\u001b[39m [\n\u001b[1;32m      7\u001b[0m     r \u001b[39mfor\u001b[39;00m r \u001b[39min\u001b[39;00m rs \u001b[39mif\u001b[39;00m r \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtypeshed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39mname\n\u001b[1;32m      8\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'small_repos' is not defined"
     ]
    }
   ],
   "source": [
    "from funs_new import count_repo_annots\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=30) as executor:\n",
    "    fs = [executor.submit(count_repo_annots, rep, repos_dir) for rep in small_repos]\n",
    "    rs = [f.result() for f in tqdm(as_completed(fs), total=len(fs))]\n",
    "useful_repos: list[GitRepo] = [\n",
    "    r for r in rs if r is not None and \"typeshed\" not in r.name\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"{len(useful_repos)}/{len(small_repos)} repos are parsable and have enough portions of type annotations.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'useful_repos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Some summary statistics\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[39m# print total number of manual annotations\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m n_total_annots \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(not_none(rep\u001b[39m.\u001b[39mn_type_annots) \u001b[39mfor\u001b[39;00m rep \u001b[39min\u001b[39;00m useful_repos)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal number of manual annotations:\u001b[39m\u001b[39m\"\u001b[39m, n_total_annots)\n\u001b[1;32m      7\u001b[0m \u001b[39m# print total number of type places\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'useful_repos' is not defined"
     ]
    }
   ],
   "source": [
    "# Some summary statistics\n",
    "\n",
    "# print total number of manual annotations\n",
    "n_total_annots = sum(not_none(rep.n_type_annots) for rep in useful_repos)\n",
    "print(\"Total number of manual annotations:\", n_total_annots)\n",
    "\n",
    "# print total number of type places\n",
    "n_total_places = sum(not_none(rep.n_type_places) for rep in useful_repos)\n",
    "print(\"Total number of type places:\", n_total_places)\n",
    "\n",
    "# print total number of lines of code\n",
    "n_total_lines = sum(not_none(rep.lines_of_code) for rep in useful_repos)\n",
    "print(\"Total number of lines of code:\", n_total_lines)\n",
    "\n",
    "# print average number of type annotations per line of code excluding projects with more than 1000 lines of code\n",
    "n_avg_annots = (\n",
    "    sum(not_none(rep.n_type_annots) for rep in useful_repos if rep.lines_of_code < 1000)\n",
    "    / n_total_lines\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 31 useful repos to /Users/iamariyap/Desktop/sem4/NLP/Final/TypeT5/scripts/useful_repos.pkl.\n",
      "[GitRepo(author='bitranox', name='lib_platform', url='https://github.com/bitranox/lib_platform', stars=0, forks=0, description='', lines_of_code=587, last_update=datetime.datetime(2022, 11, 9, 12, 46, 31), n_type_annots=57, n_type_places=57), GitRepo(author='lancelote', name='stepik_algorithms_1', url='https://github.com/lancelote/stepik_algorithms_1', stars=0, forks=0, description='', lines_of_code=583, last_update=datetime.datetime(2021, 7, 4, 17, 25, 11), n_type_annots=75, n_type_places=173), GitRepo(author='bitranox', name='lib_doctest_pycharm', url='https://github.com/bitranox/lib_doctest_pycharm', stars=0, forks=0, description='', lines_of_code=259, last_update=datetime.datetime(2021, 11, 18, 0, 46, 49), n_type_annots=21, n_type_places=21)]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "useful_repos_path = proj_root() / \"scripts\" / \"useful_repos.pkl\"\n",
    "with useful_repos_path.open(\"wb\") as f:\n",
    "    pickle.dump(useful_repos, f)\n",
    "print(f\"Saved {len(useful_repos)} useful repos to {useful_repos_path}.\")\n",
    "with useful_repos_path.open(\"rb\") as f:\n",
    "    print(pickle.load(f)[:3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
